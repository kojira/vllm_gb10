services:
  # バックエンド（vLLM + llama.cpp）
  unified-proxy:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: unified-llm-proxy
    restart: unless-stopped
    shm_size: 16g
    ipc: host
    env_file:
      - ../../.env
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      # モデルを永続化するためのボリューム
      - ./models:/workspace/models
      # Hugging Faceキャッシュ
      - ./.cache:/root/.cache
      # バックエンドコード（開発時に即座に反映）
      - ./backend:/workspace/backend
      # スクリプト
      - ./scripts:/workspace/scripts
      # データベース（会話履歴）
      - ./data:/workspace/data
      # ベンチマーク結果
      - ./benchmarks:/workspace/benchmarks
      # 旧フロントエンド（互換性のため）
      - ./frontend:/workspace/frontend
    ports:
      - "8081:8081"
    # 統合プロキシサーバーを起動（開発時は--reloadで自動リロード）
    command: >
      /bin/bash -c "WATCHFILES_FORCE_POLLING=true uvicorn backend.main:app --host 0.0.0.0 --port 8081 --reload"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # フロントエンド（React + Express）
  frontend:
    build:
      context: ./frontend-react
      dockerfile: Dockerfile
    container_name: llm-chat-frontend
    restart: unless-stopped
    environment:
      - API_TARGET=http://unified-proxy:8081
      - PORT=3000
    ports:
      - "3000:3000"
    depends_on:
      - unified-proxy
    profiles:
      - frontend
